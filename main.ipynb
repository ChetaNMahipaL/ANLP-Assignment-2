{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5UFk3gjVFdg"
      },
      "source": [
        "### **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fL5sQ-dVFdi",
        "outputId": "cc91141c-d929-4553-8a57-006ab2526c16"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-M5epDOVFdj"
      },
      "source": [
        "### **Importing and Cleaning Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsonTicJVFdk",
        "outputId": "38664624-9a67-405e-a117-3929cf622b8c"
      },
      "outputs": [],
      "source": [
        "with open('./ted-talks-corpus/train.en', 'r', encoding='utf-8') as file:\n",
        "    corpus_en = file.read()\n",
        "\n",
        "corpus_en = corpus_en.lower()\n",
        "clean_text_en = sent_tokenize(corpus_en)\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "clean_text_en = [sentence.translate(translator) for sentence in clean_text_en]\n",
        "\n",
        "with open('./ted-talks-corpus/train.fr', 'r', encoding='utf-8') as file:\n",
        "    corpus_fr = file.read()\n",
        "\n",
        "corpus_fr = corpus_fr.lower()\n",
        "clean_text_fr = sent_tokenize(corpus_fr)\n",
        "clean_text_fr = [sentence.translate(translator) for sentence in clean_text_fr]\n",
        "\n",
        "print(\"Dataset Loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdhQwLDVFdk"
      },
      "source": [
        "### **Tokenization and Emmbedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-kkc85fVFdk",
        "outputId": "1dd42ff2-5fff-415f-97fb-4e31d2a690ff"
      },
      "outputs": [],
      "source": [
        "train_data_en = [word_tokenize(sentence) for sentence in clean_text_en]\n",
        "word_to_ind_en = {}\n",
        "for i in range(len(train_data_en)):\n",
        "    token_arr = train_data_en[i]\n",
        "    \n",
        "    #Vocabulary\n",
        "    for tokken in token_arr:\n",
        "        if tokken not in word_to_ind_en:\n",
        "            word_to_ind_en[tokken] = len(word_to_ind_en)\n",
        "\n",
        "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
        "    train_data_en[i] = token_arr\n",
        "\n",
        "word_to_ind_en[\"<sos>\"] = len(word_to_ind_en)\n",
        "word_to_ind_en[\"<eos>\"] = len(word_to_ind_en)\n",
        "word_to_ind_en[\"<UNK>\"] = len(word_to_ind_en)\n",
        "\n",
        "word2vec_model_en = Word2Vec(sentences=train_data_en, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "train_data_fr = [word_tokenize(sentence) for sentence in clean_text_fr]\n",
        "word_to_ind_fr = {}\n",
        "for i in range(len(train_data_fr)):\n",
        "    token_arr = train_data_fr[i]\n",
        "    \n",
        "    #Vocabulary\n",
        "    for tokken in token_arr:\n",
        "        if tokken not in word_to_ind_fr:\n",
        "            word_to_ind_fr[tokken] = len(word_to_ind_fr)\n",
        "\n",
        "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
        "    train_data_fr[i] = token_arr\n",
        "\n",
        "word_to_ind_fr[\"<sos>\"] = len(word_to_ind_fr)\n",
        "word_to_ind_fr[\"<eos>\"] = len(word_to_ind_fr)\n",
        "word_to_ind_fr[\"<UNK>\"] = len(word_to_ind_fr)\n",
        "\n",
        "word2vec_model_fr = Word2Vec(sentences=train_data_fr, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(\"Prepare Word Embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpjaWOGJVFdk"
      },
      "source": [
        "### **Creating Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxGRwOfHVFdk",
        "outputId": "15fd07a4-bc3c-468a-86ab-63268ee181b2"
      },
      "outputs": [],
      "source": [
        "# with open('./ted-talks-corpus/train.en', 'r', encoding='utf-8') as file:\n",
        "#     corpus_en = file.read()\n",
        "\n",
        "# corpus_en = corpus_en.lower()\n",
        "# clean_text_en = sent_tokenize(corpus_en)\n",
        "# translator = str.maketrans('', '', string.punctuation)\n",
        "# clean_text_en = [sentence.translate(translator) for sentence in clean_text_en]\n",
        "\n",
        "# train_data_en = [word_tokenize(sentence) for sentence in clean_text_en]\n",
        "# for i in range(len(train_data_en)):\n",
        "#     token_arr = train_data_en[i]\n",
        "#     token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
        "#     train_data_en[i] = token_arr\n",
        "\n",
        "\n",
        "# with open('./ted-talks-corpus/train.fr', 'r', encoding='utf-8') as file:\n",
        "#     corpus_fr = file.read()\n",
        "\n",
        "# corpus_fr = corpus_fr.lower()\n",
        "# clean_text_fr = sent_tokenize(corpus_fr)\n",
        "# clean_text_fr = [sentence.translate(translator) for sentence in clean_text_fr]\n",
        "\n",
        "# train_data_fr = [word_tokenize(sentence) for sentence in clean_text_fr]\n",
        "# for i in range(len(train_data_fr)):\n",
        "#     token_arr = train_data_fr[i]\n",
        "#     token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
        "#     train_data_fr[i] = token_arr\n",
        "\n",
        "print(f\"Training data size (English): {len(train_data_en)}\")\n",
        "print(f\"Training data size (French): {len(train_data_fr)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Creating Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./ted-talks-corpus/test.en', 'r', encoding='utf-8') as file:\n",
        "    corpus_en = file.read()\n",
        "\n",
        "corpus_en = corpus_en.lower()\n",
        "clean_text_en = sent_tokenize(corpus_en)\n",
        "translator_en = str.maketrans('', '', string.punctuation)\n",
        "clean_text_en = [sentence.translate(translator) for sentence in clean_text_en]\n",
        "\n",
        "test_data_en = [word_tokenize(sentence) for sentence in clean_text_en]\n",
        "for i in range(len(test_data_en)):\n",
        "    token_arr = test_data_en[i]\n",
        "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
        "    test_data_en[i] = token_arr\n",
        "\n",
        "\n",
        "with open('./ted-talks-corpus/test.fr', 'r', encoding='utf-8') as file:\n",
        "    corpus_fr = file.read()\n",
        "\n",
        "corpus_fr = corpus_fr.lower()\n",
        "clean_text_fr = sent_tokenize(corpus_fr)\n",
        "translator_fr = str.maketrans('', '', string.punctuation)\n",
        "clean_text_fr = [sentence.translate(translator) for sentence in clean_text_fr]\n",
        "\n",
        "test_data_fr = [word_tokenize(sentence) for sentence in clean_text_fr]\n",
        "for i in range(len(test_data_fr)):\n",
        "    token_arr = test_data_fr[i]\n",
        "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
        "    test_data_fr[i] = token_arr\n",
        "\n",
        "print(f\"Testing data size (English): {len(test_data_en)}\")\n",
        "print(f\"Testing data size (French): {len(test_data_fr)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5AGJfKQVFdl"
      },
      "source": [
        "## **Encoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaOuRTZUVFdl"
      },
      "source": [
        "### **Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3oAGT49VFdl"
      },
      "outputs": [],
      "source": [
        "class PosEncoding(nn.Module):\n",
        "    def __init__(self, model_dim, max_len):\n",
        "        super(PosEncoding, self).__init__()\n",
        "        pos_code = torch.zeros(max_len, model_dim).to(device)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1).to(device)\n",
        "        scale = torch.exp(torch.arange(0, model_dim, 2, dtype=torch.float) *\n",
        "                          -(math.log(10000.0) / model_dim)).to(device)\n",
        "\n",
        "        pos_code[:, 0::2] = torch.sin(pos * scale)\n",
        "        pos_code[:, 1::2] = torch.cos(pos * scale)\n",
        "\n",
        "        self.register_buffer('pos_code', pos_code.unsqueeze(0))\n",
        "\n",
        "    def forward(self, inp):\n",
        "        seq_len = inp.size(1)\n",
        "        inp = inp.to(device) + self.pos_code[:, :seq_len]\n",
        "\n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFmGvWLHVFdl"
      },
      "source": [
        "### **Multi Head Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFbih_nXVFdl"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_key = self.model_dim // self.num_heads\n",
        "\n",
        "        # Linear layers for query, key, and value\n",
        "        self.query = nn.Linear(model_dim, model_dim).to(device)\n",
        "        self.key = nn.Linear(model_dim, model_dim).to(device)\n",
        "        self.value = nn.Linear(model_dim, model_dim).to(device)\n",
        "        self.out = nn.Linear(model_dim, model_dim).to(device)\n",
        "\n",
        "    def attention_val(self, Q, K, V, mask=None):\n",
        "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dim_key)\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weight = torch.softmax(score, dim=-1)\n",
        "\n",
        "        new_val = torch.matmul(attn_weight, V)\n",
        "        return new_val\n",
        "\n",
        "    def split_layers(self, x):\n",
        "        batch_size, seq_len, model_dim = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.dim_key).transpose(1, 2)\n",
        "\n",
        "    def combine_layers(self, x):\n",
        "        batch_size, _, seq_len, dim_key = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.model_dim)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Split into multiple heads\n",
        "        Q = self.split_layers(self.query(Q).to(device))\n",
        "        K = self.split_layers(self.key(K).to(device))\n",
        "        V = self.split_layers(self.value(V).to(device))\n",
        "\n",
        "        layer_out = self.attention_val(Q, K, V, mask)\n",
        "        final_output = self.out(self.combine_layers(layer_out)).to(device)\n",
        "\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuueGbFIVFdm"
      },
      "source": [
        "### **FeedForward**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYifHE8fVFdm"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, model_dim, hid_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.l1 = nn.Linear(model_dim, hid_dim).to(device)\n",
        "        self.ac1 = nn.ReLU().to(device)\n",
        "        self.l2 = nn.Linear(hid_dim, model_dim).to(device)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        inp = self.l1(inp).to(device)\n",
        "        inp = self.ac1(inp).to(device)\n",
        "        inp = self.l2(inp).to(device)\n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Combining Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads, hid_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(model_dim, num_heads).to(device)\n",
        "        self.norm = nn.LayerNorm(model_dim).to(device)\n",
        "        self.ffn = FeedForward(model_dim, hid_dim).to(device)\n",
        "        self.dropout = nn.Dropout(dropout).to(device)\n",
        "\n",
        "    def forward(self, inp, mask):\n",
        "\n",
        "        att_score = self.self_attn(inp, inp, inp, mask).to(device)\n",
        "        inp = self.norm(inp + self.dropout(att_score).to(device)).to(device)\n",
        "        ffn_out = self.ffn(inp).to(device)\n",
        "        inp = self.norm(inp + self.dropout(ffn_out).to(device)).to(device)\n",
        "\n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx4WjCqBVFdm"
      },
      "source": [
        "## **Decoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmTj8mXYVFdm"
      },
      "source": [
        "### **Combining Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frJba1nzVFdn"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads, hid_dim, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(model_dim, num_heads).to(device)\n",
        "        self.cr_attn = MultiHeadAttention(model_dim, num_heads).to(device)\n",
        "        self.norm = nn.LayerNorm(model_dim).to(device)\n",
        "        self.ffn = FeedForward(model_dim, hid_dim).to(device)\n",
        "        self.dropout = nn.Dropout(dropout).to(device)\n",
        "\n",
        "    def forward(self, inp, enc_output, src_mask, target_mask):\n",
        "\n",
        "        att_score = self.self_attn(inp, inp, inp, target_mask)\n",
        "        if isinstance(att_score, tuple):\n",
        "            att_score = att_score[0]\n",
        "        inp = self.norm(inp + self.dropout(att_score)).to(device)\n",
        "        attn_score = self.cr_attn(inp, enc_output, enc_output, src_mask)\n",
        "        inp = self.norm(inp + self.dropout(attn_score)).to(device)\n",
        "        ffn_out = self.ffn(inp).to(device)\n",
        "        inp = self.norm(inp + self.dropout(ffn_out).to(device)).to(device)\n",
        "        \n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qUfIVGVFdn"
      },
      "source": [
        "## **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq1O4ZHlVFdn"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size_en, vocab_size_fr, model_dim, num_layer, num_heads, hid_dim, max_len, dropout=0.1, pretrained_embeddings_en=None, pretrained_embeddings_fr=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        # Embeddings for English\n",
        "        if pretrained_embeddings_en is not None:\n",
        "            self.embed_en = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings_en, dtype=torch.float), freeze=True)\n",
        "        else:\n",
        "            self.embed_en = nn.Embedding(vocab_size_en, model_dim)\n",
        "        # Embeddings for French\n",
        "        if pretrained_embeddings_fr is not None:\n",
        "            self.embed_fr = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings_fr, dtype=torch.float), freeze=True)\n",
        "        else:\n",
        "            self.embed_fr = nn.Embedding(vocab_size_fr, model_dim)\n",
        "\n",
        "        self.pos_enc = PosEncoding(model_dim, max_len)\n",
        "        self.encoders = nn.ModuleList([Encoder(model_dim, num_heads, hid_dim, dropout) for _ in range(num_layer)])\n",
        "        self.decoders = nn.ModuleList([Decoder(model_dim, num_heads, hid_dim, dropout) for _ in range(num_layer)])\n",
        "        self.final_layer = nn.Linear(model_dim, vocab_size_fr)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, src, target):\n",
        "        src = src.to(device)\n",
        "        src_mask = self.generate_square_subsequent_mask(target.size(0),target.size(1)).to(device)\n",
        "        src_emb = self.pos_enc(self.embed_en(src))\n",
        "        target_emb = self.pos_enc(self.embed_fr(target))\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            src_emb = encoder(src_emb, src_mask)\n",
        "\n",
        "        for decoder in self.decoders:\n",
        "            target_emb = decoder(target_emb, src_mask)\n",
        "\n",
        "        return self.final_layer(target_emb)\n",
        "\n",
        "    def generate_square_subsequent_mask(self,src, sz, sz_1):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        mask = (torch.triu(torch.ones(sz, sz_1)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return src_mask, mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDJoaqvdVFdn"
      },
      "source": [
        "### **Creating Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1KPf2g_VFdn"
      },
      "outputs": [],
      "source": [
        "class LM_Dataset(torch.utils.data.Dataset):\n",
        "    # def __init__(self, data, seq_len):\n",
        "    #     self.data = data\n",
        "    #     self.seq_len = seq_len\n",
        "\n",
        "    # def __len__(self):\n",
        "    #     return (self.data.size(0) - 1) // self.seq_len\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     start = idx * self.seq_len\n",
        "    #     end = start + self.seq_len\n",
        "    #     src = self.data[start:end]\n",
        "    #     target = self.data[start+1:end+1]\n",
        "    #     return src, target\n",
        "    def __init__(self, src_data, tgt_data, src_seq_len, tgt_seq_len):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "        self.src_seq_len = src_seq_len\n",
        "        self.tgt_seq_len = tgt_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        src = self.src_data[idx][:self.src_seq_len]\n",
        "        tgt = self.tgt_data[idx][:self.tgt_seq_len]\n",
        "        \n",
        "        tgt_input = tgt[:-1]\n",
        "        tgt_output = tgt[1:]\n",
        "\n",
        "        return src, tgt_input, tgt_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYp-ZUfmVFdn"
      },
      "source": [
        "### **Creating Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGlKjcNzVFdn",
        "outputId": "65093871-e966-4d04-cee0-bede0c2bb245"
      },
      "outputs": [],
      "source": [
        "def prepare_data(sentences, word_to_index, max_len=None):\n",
        "    def words_to_indices(words, word_to_index):\n",
        "        return [word_to_index.get(word, 0) for word in words]\n",
        "\n",
        "    all_indices = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_indices = words_to_indices(sentence, word_to_index)\n",
        "\n",
        "        if max_len is not None:\n",
        "            word_indices = word_indices[:max_len]\n",
        "\n",
        "        all_indices.extend(word_indices)\n",
        "\n",
        "    data = torch.LongTensor(all_indices)\n",
        "    return data\n",
        "\n",
        "train_gram_inp_en = prepare_data(train_data_en, word_to_ind_en, max_len=40)\n",
        "train_gram_inp_fr = prepare_data(train_data_fr, word_to_ind_en, max_len=40)\n",
        "test_gram_inp = prepare_data(test_data_fr, word_to_ind_fr, max_len=40)\n",
        "\n",
        "# Example output\n",
        "print(\"Created input for loading\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGNFQRZ5VFdo"
      },
      "source": [
        "### **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xF5uSy-VFdo",
        "outputId": "3b644a45-b5f6-453a-823e-084b3915fe7f"
      },
      "outputs": [],
      "source": [
        "print(\"Training Begins\")\n",
        "\n",
        "dataset_train = LM_Dataset(train_gram_inp_en, train_gram_inp_fr,40, 40)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
        "\n",
        "pretrained_embeddings_en = word2vec_model_en.wv.vectors\n",
        "pretrained_embeddings_fr = word2vec_model_fr.wv.vectors\n",
        "\n",
        "model = Transformer(vocab_size_en=len(word_to_ind_en), vocab_size_fr=len(word_to_ind_fr), \n",
        "                        model_dim=100, num_heads=4, num_layer=6, hid_dim=300, max_len=40, dropout=0.1, \n",
        "                        pretrained_embeddings_en=pretrained_embeddings_en, pretrained_embeddings_fr=pretrained_embeddings_fr)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "perp_vis_t = []\n",
        "perp_vis_val = []\n",
        "\n",
        "with open('2022101096_LM3_Train_Perplexity.txt', 'w') as train_file, open('2022101096_LM3_Validation_Perplexity.txt', 'w') as val_file:\n",
        "\n",
        "    train_file.write(f'Epoch\\tBatch\\tPerplexity\\n')\n",
        "    val_file.write(f'Epoch\\tBatch\\tPerplexity\\n')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_index, batch in enumerate(dataloader_train):\n",
        "            context_words, target_words_inp, target_words_out = batch\n",
        "            context_words = context_words.to(device)\n",
        "            target_words_inp = target_words_inp.to(device)\n",
        "            target_words_out = target_words_out.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(context_words, target_words_inp)\n",
        "            # print(f\"Outputs shape: {outputs.shape}\")\n",
        "            # print(f\"Outputs sample: {outputs[0, :5]}\")\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.size(-1))\n",
        "            target_words = target_words.view(-1)\n",
        "\n",
        "            loss = criterion(outputs, target_words_out)\n",
        "            # print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_perplexity_t = math.exp(loss.item())\n",
        "\n",
        "            train_file.write(f'{epoch+1}\\t{batch_index+1}\\t{batch_perplexity_t:.4f}\\n')\n",
        "\n",
        "        avg_train_loss = total_loss / len(dataloader_train)\n",
        "        train_perplexity = math.exp(avg_train_loss)\n",
        "\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #     for batch in dataloader_val:\n",
        "        #         context_words, target_words = batch\n",
        "        #         context_words = context_words.to(device)\n",
        "        #         target_words = target_words.to(device)\n",
        "\n",
        "        #         outputs = model(context_words)\n",
        "        #         outputs = outputs.view(-1, outputs.size(-1))\n",
        "        #         target_words = target_words.view(-1)\n",
        "        #         loss = criterion(outputs, target_words)\n",
        "        #         total_val_loss += loss.item()\n",
        "        #         batch_perplexity = math.exp(loss.item())\n",
        "\n",
        "        #         val_file.write(f'{epoch+1}\\t{batch_index+1}\\t{batch_perplexity:.4f}\\n')\n",
        "\n",
        "        #         _, predicted = torch.max(outputs, 1)\n",
        "        #         total += target_words.size(0)\n",
        "        #         correct += (predicted == target_words).sum().item()\n",
        "\n",
        "\n",
        "        # avg_val_loss = total_val_loss / len(dataloader_val)\n",
        "        # val_perplexity = math.exp(avg_val_loss)\n",
        "        # accuracy = 100 * correct / total\n",
        "        # perp_vis_t.append(train_perplexity)\n",
        "        # perp_vis_val.append(val_perplexity)\n",
        "\n",
        "        train_file.write(f'End of Epoch {epoch+1} - Average Train Perplexity: {train_perplexity:.4f}\\n')\n",
        "        # val_file.write(f'End of Epoch {epoch+1} - Average Val Perplexity: {val_perplexity:.4f}\\n')\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}')\n",
        "        # , Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%'\n",
        "\n",
        "print(\"Training and Validation Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI_4BWWCVFdo"
      },
      "source": [
        "### **Evaluate Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6obGWhHdVFdo",
        "outputId": "e42d228a-455c-4b6e-f249-45ce9a572a7b"
      },
      "outputs": [],
      "source": [
        "print(\"Testing Begins\")\n",
        "\n",
        "dataset_test = LM_Dataset(test_gram_inp, 40)\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=16)\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "total_loss = 0\n",
        "total_tokens = 0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "with open('2022101096_LM3_Test_Perplexity.txt', 'w') as f:\n",
        "    f.write(f'Batch\\tPerplexity\\n')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch in enumerate(dataloader_test):\n",
        "            context_words, target_words = batch\n",
        "            context_words = context_words.to(device)\n",
        "            target_words = target_words.to(device)\n",
        "\n",
        "            outputs = model(context_words)\n",
        "            outputs = outputs.view(-1, outputs.size(-1))\n",
        "            target_words = target_words.view(-1)\n",
        "\n",
        "            loss = criterion(outputs, target_words)\n",
        "            total_loss += loss.item()\n",
        "            perplexity = math.exp(loss.item())\n",
        "\n",
        "            f.write(f'{batch_index+1}\\t{perplexity:.4f}\\n')\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target_words.size(0)\n",
        "            correct += (predicted == target_words).sum().item()\n",
        "\n",
        "        average_perplexity = math.exp(total_loss / len(dataloader_test))\n",
        "        f.write(f'Average perplexity:\\t{average_perplexity:.4f}\\n')\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "average_loss = total_loss / len(dataloader_test)\n",
        "perplexity = math.exp(average_loss)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "print(f'Average Test Loss: {average_loss:.4f}')\n",
        "print(f'Perplexity: {perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "jNtM0xbNYZ0h",
        "outputId": "a9c57e1c-e393-4412-aed2-17dc7228f753"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(perp_vis_t) + 1)  # Assuming each entry corresponds to an epoch\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, perp_vis_t, marker='o', linestyle='-', color='blue', label='Training Perplexity')\n",
        "plt.plot(epochs, perp_vis_val, marker='o', linestyle='--', color='red', label='Validation Perplexity')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.title('Training and Validation Perplexity Over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig('plot.png')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZqVU6ISYbjU"
      },
      "outputs": [],
      "source": [
        "# Save the entire model\n",
        "torch.save(model, './model/model_LM3.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
