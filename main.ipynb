{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5UFk3gjVFdg"
      },
      "source": [
        "### **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fL5sQ-dVFdi",
        "outputId": "028dc77e-8f6a-4854-968f-1e520a9d9915"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import torch.optim as optim\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction \n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-M5epDOVFdj"
      },
      "source": [
        "### **Importing and Cleaning Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Loading Corpus**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsonTicJVFdk",
        "outputId": "1d981796-b29b-410d-bf66-5982305301cb"
      },
      "outputs": [],
      "source": [
        "def retrieve_corpus(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        corpus = []\n",
        "        for line in file:\n",
        "            # print(line.strip().lower())\n",
        "            corpus.append(line.lower())\n",
        "        return corpus\n",
        "\n",
        "corpus_en = retrieve_corpus(\"./ted-talks-corpus/train.en\")\n",
        "corpus_fr = retrieve_corpus(\"./ted-talks-corpus/train.fr\")\n",
        "corpus_en_val = retrieve_corpus(\"./ted-talks-corpus/dev.en\")\n",
        "corpus_fr_val = retrieve_corpus(\"./ted-talks-corpus/dev.fr\")\n",
        "corpus_en_test = retrieve_corpus(\"./ted-talks-corpus/test.en\")\n",
        "corpus_fr_test = retrieve_corpus(\"./ted-talks-corpus/test.fr\")\n",
        "\n",
        "\n",
        "print(\"Dataset Loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdhQwLDVFdk"
      },
      "source": [
        "### **Tokenization and Emmbedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **For English**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_punctuation(tokenized_sentence):\n",
        "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
        "\n",
        "train_data_en = [remove_punctuation(word_tokenize(sentence)) for sentence in corpus_en]\n",
        "val_data_en = [remove_punctuation(word_tokenize(sentence)) for sentence in corpus_en_val]\n",
        "\n",
        "# Replacing words with frequency less than 5 with <unk>\n",
        "word_freq={}\n",
        "threshold = 3\n",
        "for _, sentence in enumerate(train_data_en):\n",
        "    for word in sentence:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "for _, sentence in enumerate(val_data_en):\n",
        "    for word in sentence:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "vocab_en = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "for i, sentence in enumerate(train_data_en):\n",
        "    for j,word in enumerate(sentence):\n",
        "        if word_freq[word] < threshold:\n",
        "            train_data_en[i][j] = \"<unk>\"\n",
        "\n",
        "for i, sentence in enumerate(val_data_en):\n",
        "    for j,word in enumerate(sentence):\n",
        "        if word_freq[word] < threshold:\n",
        "            val_data_en[i][j] = \"<unk>\"\n",
        "\n",
        "# Creating Vocabulary of English\n",
        "for word in word_freq:\n",
        "    if word_freq[word]>=threshold:\n",
        "        vocab_en.append(word)\n",
        "\n",
        "word2idx_en = {}\n",
        "idx2word_en = {}\n",
        "for i, word in enumerate(vocab_en):\n",
        "    word2idx_en[word] = i\n",
        "    idx2word_en[i] = word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **For French**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_fr = [remove_punctuation(word_tokenize(sentence)) for sentence in corpus_fr]\n",
        "val_data_fr = [remove_punctuation(word_tokenize(sentence)) for sentence in corpus_fr_val]\n",
        "\n",
        "\n",
        "# Replacing words with frequency less than 5 with <unk>\n",
        "word_freq={}\n",
        "threshold = 3\n",
        "for _, sentence in enumerate(train_data_fr):\n",
        "    for word in sentence:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "for _, sentence in enumerate(val_data_fr):\n",
        "    for word in sentence:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "vocab_fr = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "for i, sentence in enumerate(train_data_fr):\n",
        "    for j,word in enumerate(sentence):\n",
        "        if word_freq[word] < threshold:\n",
        "            train_data_fr[i][j] = \"<unk>\"\n",
        "\n",
        "for i, sentence in enumerate(val_data_fr):\n",
        "    for j,word in enumerate(sentence):\n",
        "        if word_freq[word] < threshold:\n",
        "            val_data_fr[i][j] = \"<unk>\"\n",
        "\n",
        "# Creating Vocabulary of French\n",
        "for word in word_freq:\n",
        "    if word_freq[word]>=threshold:\n",
        "        vocab_fr.append(word)\n",
        "\n",
        "word2idx_fr = {}\n",
        "idx2word_fr = {}\n",
        "for i, word in enumerate(vocab_fr):\n",
        "    word2idx_fr[word] = i\n",
        "    idx2word_fr[i] = word\n",
        "print(len(word2idx_en))\n",
        "print(len(word2idx_fr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpjaWOGJVFdk"
      },
      "source": [
        "### **Creating Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxGRwOfHVFdk",
        "outputId": "c6b5a7ff-7d7a-49f0-d34a-fd5991872e52"
      },
      "outputs": [],
      "source": [
        "print(f\"Training data size (English): {len(train_data_en)}\")\n",
        "print(f\"Training data size (French): {len(train_data_fr)}\")\n",
        "print(f\"Validating data size (English): {len(val_data_en)}\")\n",
        "print(f\"Validating data size (French): {len(val_data_fr)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRw9RHd-wTil"
      },
      "source": [
        "### **Creating Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_12tIsx0wTil",
        "outputId": "83305c4a-4ac8-4e41-a710-266e9b09acf6"
      },
      "outputs": [],
      "source": [
        "test_data_en = [remove_punctuation(word_tokenize(sentence)) for sentence in corpus_en_test]\n",
        "test_data_fr = [remove_punctuation(word_tokenize(sentence)) for sentence in corpus_fr_test]\n",
        "\n",
        "print(f\"Testing data size (English): {len(test_data_en)}\")\n",
        "print(f\"Testing data size (French): {len(test_data_fr)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaOuRTZUVFdl"
      },
      "source": [
        "### **Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3oAGT49VFdl"
      },
      "outputs": [],
      "source": [
        "class PosEncoding(nn.Module):\n",
        "    def __init__(self, model_dim, max_len):\n",
        "        super(PosEncoding, self).__init__()\n",
        "        pos_code = torch.zeros(max_len, model_dim)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        scale = torch.exp(torch.arange(0, model_dim, 2, dtype=torch.float) *\n",
        "                          -(math.log(10000.0) / model_dim))\n",
        "\n",
        "        pos_code[:, 0::2] = torch.sin(pos * scale)\n",
        "        pos_code[:, 1::2] = torch.cos(pos * scale)\n",
        "\n",
        "        self.register_buffer('pos_code', pos_code.unsqueeze(0))\n",
        "\n",
        "    def forward(self, inp):\n",
        "        seq_len = inp.size(1)\n",
        "        inp = inp + self.pos_code[:, :seq_len]\n",
        "\n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFmGvWLHVFdl"
      },
      "source": [
        "### **Multi Head Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFbih_nXVFdl"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_key = self.model_dim // self.num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Linear layers for query, key, and value\n",
        "        self.query = nn.Linear(model_dim, model_dim)\n",
        "        self.key = nn.Linear(model_dim, model_dim)\n",
        "        self.value = nn.Linear(model_dim, model_dim)\n",
        "        self.out = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "    def attention_val(self, Q, K, V, mask=None):\n",
        "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dim_key)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weight = self.dropout(torch.softmax(score, dim=-1))\n",
        "\n",
        "        new_val = torch.matmul(attn_weight, V)\n",
        "        return new_val\n",
        "\n",
        "    def split_layers(self, x):\n",
        "        batch_size, seq_len, model_dim = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.dim_key).transpose(1, 2)\n",
        "\n",
        "    def combine_layers(self, x):\n",
        "        batch_size, _, seq_len, dim_key = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.model_dim)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Split into multiple heads\n",
        "        Q = self.split_layers(self.query(Q))\n",
        "        K = self.split_layers(self.key(K))\n",
        "        V = self.split_layers(self.value(V))\n",
        "\n",
        "        layer_out = self.attention_val(Q, K, V, mask)\n",
        "        final_output = self.out(self.combine_layers(layer_out))\n",
        "\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuueGbFIVFdm"
      },
      "source": [
        "### **FeedForward**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYifHE8fVFdm"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, model_dim, hid_dim, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.l1 = nn.Linear(model_dim, hid_dim)\n",
        "        self.ac1 = nn.ReLU().to(device)\n",
        "        self.l2 = nn.Linear(hid_dim, model_dim)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        inp = self.l1(inp)\n",
        "        inp = self.dropout(self.ac1(inp))\n",
        "        inp = self.l2(inp)\n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L8L_u0ywTim"
      },
      "source": [
        "### **Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJbi3aMIwTim"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads, hid_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(model_dim, num_heads,dropout)\n",
        "        self.norm = nn.LayerNorm(model_dim)\n",
        "        self.ffn = FeedForward(model_dim, hid_dim,dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp, mask):\n",
        "\n",
        "        att_score = self.self_attn(inp, inp, inp, mask)\n",
        "        inp = self.norm(inp + self.dropout(att_score))\n",
        "        ffn_out = self.ffn(inp).to(device)\n",
        "        inp = self.norm(inp + self.dropout(ffn_out))\n",
        "\n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx4WjCqBVFdm"
      },
      "source": [
        "### **Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frJba1nzVFdn"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads, hid_dim, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(model_dim, num_heads,dropout)\n",
        "        self.cr_attn = MultiHeadAttention(model_dim, num_heads,dropout)\n",
        "        self.norm = nn.LayerNorm(model_dim)\n",
        "        self.ffn = FeedForward(model_dim, hid_dim,dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp, enc_output, src_mask, target_mask):\n",
        "\n",
        "        att_score = self.self_attn(inp, inp, inp, target_mask)\n",
        "        if isinstance(att_score, tuple):\n",
        "                att_score = att_score[0]\n",
        "        inp = self.norm(inp + self.dropout(att_score))\n",
        "        attn_score = self.cr_attn(inp, enc_output, enc_output, src_mask)\n",
        "        inp = self.norm(inp + self.dropout(attn_score))\n",
        "        ffn_out = self.ffn(inp)\n",
        "        inp = self.norm(inp + self.dropout(ffn_out).to(device))\n",
        "\n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qUfIVGVFdn"
      },
      "source": [
        "### **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq1O4ZHlVFdn"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size_en, vocab_size_fr, model_dim, num_layer, num_heads, hid_dim, max_len, dropout=0.1, pretrained_embeddings_en=None, pretrained_embeddings_fr=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        if pretrained_embeddings_en is not None:\n",
        "            self.embed_en = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings_en, dtype=torch.float), freeze=True)\n",
        "        else:\n",
        "            self.embed_en = nn.Embedding(vocab_size_en, model_dim)\n",
        "        # Embeddings for French\n",
        "        if pretrained_embeddings_fr is not None:\n",
        "            self.embed_fr = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings_fr, dtype=torch.float), freeze=True)\n",
        "        else:\n",
        "            self.embed_fr = nn.Embedding(vocab_size_fr, model_dim)\n",
        "\n",
        "        self.pos_enc = PosEncoding(model_dim, max_len)\n",
        "        self.encoders = nn.ModuleList([Encoder(model_dim, num_heads, hid_dim, dropout) for _ in range(num_layer)])\n",
        "        self.decoders = nn.ModuleList([Decoder(model_dim, num_heads, hid_dim, dropout) for _ in range(num_layer)])\n",
        "        self.final_layer = nn.Linear(model_dim, vocab_size_fr)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def decode(self, src, sos_token, eos_token, max_len=40):\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        src_emb = self.pos_enc(self.embed_en(src))\n",
        "        for encoder in self.encoders:\n",
        "            src_emb = encoder(src_emb, None)\n",
        "\n",
        "        tgt = torch.ones(batch_size, 1).long().to(device)\n",
        "\n",
        "\n",
        "        for _ in range(max_len):\n",
        "\n",
        "            src_mask, tgt_mask = self.generate_square_subsequent_mask(src, tgt)\n",
        "            tgt_emb = self.pos_enc(self.embed_fr(tgt))\n",
        "            for decoder in self.decoders:\n",
        "                tgt_emb = decoder(tgt_emb, src_emb, src_mask, tgt_mask)\n",
        "\n",
        "            output = self.final_layer(tgt_emb)\n",
        "            next_word = output[:, -1:, :]\n",
        "            next_word = torch.argmax(next_word, dim=-1)\n",
        "            tgt = torch.cat((tgt, next_word), dim=1)\n",
        "            # tgt = out_labels\n",
        "\n",
        "        return tgt\n",
        "\n",
        "\n",
        "    def forward(self, src, target):\n",
        "        src = src\n",
        "        src_mask, target_mask = self.generate_square_subsequent_mask(src, target)\n",
        "        src_mask = src_mask\n",
        "        target_mask = target_mask\n",
        "        src_emb = self.pos_enc(self.embed_en(src))\n",
        "        target_emb = self.pos_enc(self.embed_fr(target))\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            src_emb = encoder(src_emb, src_mask)\n",
        "\n",
        "        for decoder in self.decoders:\n",
        "            target_emb = decoder(target_emb, src_emb, src_mask, target_mask)\n",
        "\n",
        "        return self.final_layer(target_emb)\n",
        "\n",
        "    def generate_square_subsequent_mask(self,src, target):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (target != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt_mask.size(2)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask.to(device)\n",
        "        return src_mask, tgt_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDJoaqvdVFdn"
      },
      "source": [
        "### **Creating Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1KPf2g_VFdn"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_data[idx]\n",
        "        tgt_input = self.tgt_data[idx]\n",
        "\n",
        "        return src, tgt_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYp-ZUfmVFdn"
      },
      "source": [
        "### **Creating Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGlKjcNzVFdn",
        "outputId": "c16abca8-78ca-40e9-b4e4-ad34d8e4ccd2"
      },
      "outputs": [],
      "source": [
        "def prepare_data(sentences, word2idx,max_len=100):\n",
        "    for i, sentence in enumerate(sentences):\n",
        "\n",
        "        if len(sentence) > max_len-2:\n",
        "            sentences[i] = sentence[:max_len-2]\n",
        "            sentences[i] = [\"<sos>\"] + sentences[i] + [\"<eos>\"]\n",
        "        else:\n",
        "            sentences[i] = [\"<sos>\"] + sentences[i] + [\"<eos>\"]\n",
        "    \n",
        "    all_indices = []\n",
        "    for sentence in sentences:\n",
        "        index = []\n",
        "        for word in sentence:\n",
        "            if word in word2idx:\n",
        "                index.append(word2idx[word])\n",
        "            else:\n",
        "                index.append(word2idx[\"<unk>\"])\n",
        "\n",
        "        index = index + [word2idx[\"<pad>\"]] * (max_len-len(index))\n",
        "        index = torch.tensor(index)\n",
        "        all_indices.append(index)  \n",
        "        \n",
        "    return all_indices\n",
        "\n",
        "# Now prepare the datasets\n",
        "trainIdx_en = prepare_data(train_data_en, word2idx_en)\n",
        "trainIdx_fr = prepare_data(train_data_fr, word2idx_fr)\n",
        "valIdx_en = prepare_data(val_data_en, word2idx_en)\n",
        "valIdx_fr = prepare_data(val_data_fr, word2idx_fr)\n",
        "testIdx_en = prepare_data(test_data_en, word2idx_en)\n",
        "testIdx_fr = prepare_data(test_data_fr, word2idx_fr)\n",
        "\n",
        "# Example output\n",
        "print(\"Created input for loading\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGNFQRZ5VFdo"
      },
      "source": [
        "### **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "8xF5uSy-VFdo",
        "outputId": "d4048ae2-7250-486c-b8f3-f1ffdaf67d45"
      },
      "outputs": [],
      "source": [
        "print(\"Training Begins\")\n",
        "\n",
        "dataset_train = TranslationDataset(trainIdx_en, trainIdx_fr)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
        "\n",
        "dataset_val = TranslationDataset(valIdx_en, valIdx_fr)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=16, shuffle=True)\n",
        "\n",
        "model = Transformer(vocab_size_en=len(word2idx_en), vocab_size_fr=len(word2idx_fr),\n",
        "                        model_dim=100, num_heads=4, num_layer=4, hid_dim=300, max_len=100, dropout=0.1)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = word2idx_fr[\"<pad>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "perp_vis_t = []\n",
        "perp_vis_val = []\n",
        "\n",
        "    \n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_index, batch in enumerate(dataloader_train):\n",
        "        context_words, target_words = batch\n",
        "        context_words = context_words.to(device)\n",
        "        target_words = target_words.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(context_words, target_words[:, :-1])\n",
        "\n",
        "        outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
        "        target_words_out = target_words[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(outputs, target_words_out)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(dataloader_train)\n",
        "\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader_val:\n",
        "            context_words, target_words = batch\n",
        "            context_words = context_words.to(device)\n",
        "            target_words = target_words.to(device)\n",
        "\n",
        "            outputs = model(context_words, target_words[:, :-1])\n",
        "\n",
        "            outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
        "            target_words_out = target_words[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(outputs, target_words_out)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            \n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target_words_out.size(0)\n",
        "            correct += (predicted == target_words_out).sum().item()\n",
        "\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(dataloader_val)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "print(\"Training and Validation Complete.\")\n",
        "# torch.save(model, './model.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI_4BWWCVFdo"
      },
      "source": [
        "### **Evaluate Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "6obGWhHdVFdo",
        "outputId": "91d6510d-d86a-4ded-c3ff-208c1b0162e5"
      },
      "outputs": [],
      "source": [
        "def smoothedBleu(reference, candidate):\n",
        "    smoothing_function = SmoothingFunction().method7\n",
        "    return sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
        "\n",
        "print(\"Testing Begins\")\n",
        "\n",
        "dataset_test = TranslationDataset(testIdx_en, testIdx_fr)\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=16)\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "total_loss = 0\n",
        "total_tokens = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = word2idx_fr[\"<pad>\"])\n",
        "hypotheses = []\n",
        "references = []\n",
        "avg_bleu_score = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_index, batch in enumerate(dataloader_test):\n",
        "        context_words, target_words= batch\n",
        "        context_words = context_words.to(device)\n",
        "        target_words = target_words.to(device)\n",
        "\n",
        "        sos_token_id = word2idx_en[\"<sos>\"]\n",
        "        eos_token_id = word2idx_en[\"<eos>\"]\n",
        "        outputs = model(context_words, target_words[:, :-1])\n",
        "\n",
        "        outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
        "        target_words_out = target_words[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(outputs, target_words_out)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += target_words_out.size(0)\n",
        "        correct += (predicted == target_words_out).sum().item()\n",
        "\n",
        "        generated = model.decode(context_words, sos_token_id, eos_token_id, 100)\n",
        "        for hyp, ref in zip(generated, target_words):\n",
        "            hyp = hyp.tolist()\n",
        "            ref = ref.tolist()\n",
        "\n",
        "            hyp_words = [idx2word_fr[token] for token in hyp\n",
        "                          if token not in {0, sos_token_id,eos_token_id}]\n",
        "            print(hyp_words)\n",
        "            # print(\"Break\")\n",
        "            ref_words = [idx2word_fr[token] for token in ref\n",
        "                          if token not in {0, sos_token_id,eos_token_id}]\n",
        "            print(ref_words)\n",
        "            print(\"Next\")\n",
        "        hypotheses.append(hyp_words)\n",
        "        references.append(ref_words)\n",
        "            \n",
        "        avg_bleu_score += smoothedBleu(ref_words, hyp_words)\n",
        "            # print(\"Next\")\n",
        "\n",
        "\n",
        "\n",
        "    print(f'The avg Bleu Score is : {avg_bleu_score/len(dataloader_test)}')\n",
        "# for i in range(1, 5):\n",
        "#     weights = [1/i] * i + [0] * (4-i)\n",
        "#     bleu_score = corpus_bleu(references, hypotheses, weights=weights)\n",
        "#     print(f\"BLEU-{i} score: {bleu_score}\")\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "average_loss = total_loss / len(dataloader_test)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "print(f'Average Test Loss: {average_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNtM0xbNYZ0h"
      },
      "outputs": [],
      "source": [
        "print(word2idx_en[\"26\"])\n",
        "print(word2idx_fr[\"j'avais\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZqVU6ISYbjU"
      },
      "outputs": [],
      "source": [
        "# Save the entire model\n",
        "# torch.save(model, './model.pth')\n",
        "# model.cpu()  # Move the model to the CPU\n",
        "# torch.save(model, './model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
